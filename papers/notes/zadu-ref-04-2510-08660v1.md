---
id: paper-2024-zadu-ref-04
title: "How Scale Breaks ‚ÄúNormalized Stress‚Äù and KL Divergence: Rethinking Quality Metrics"
authors: "Kiran Smelser, Kaviru Gunaratne, Jacob Miller, Stephen Kobourov"
venue: "IEEE Transactions on Visualization and Computer Graphics"
year: 2024
tags: [dr, zadu-table1-reference, dsc, kl_div, mrre, nh, nm_stress, pr, sn_stress, snc, srho, stress, tnc]
source_pdf: papers/raw/zadu-table1-references/2510.08660v1.pdf
evidence_level: medium
updated_at: 2026-02-08
---
# Problem
- DR Techniques Embedding a sufficiently complex high-dimensional dataset into a lower-dimensional space unavoidably causes distortion, which results in the loss of some information in the dataset [11].
- In particular, our contributions are: ‚Ä¢ Showing analytically and empirically that normalized stress and KL divergence are severely affected by scale and can lead to incorrect conclusions ‚Ä¢ Demonstrating that using a scale-invariant stress measure impacts the results of previous work ‚Ä¢ Proposing scale-invariant options for stress and KL divergence We also provide an interactive webpage that demonstrates the problems discussed here with 6 datasets.
- We consider a dataset of 3 samples, whose high-dimensional probability distribution we set as follows: P= Ô£Æ Ô£∞ 0 0.2 0.1 0.2 0 0.2 0.1 0.2 0 Ô£π Ô£ª For this dataset, we create the following embedding: Y=  0 0  ,  0 1  ,  1 1  The following graph for the corresponding KL divergence function clearly depicts KL divergence approaching its horizontal asymptote of 0 from above.
- Evaluating with KL divergence provides a measure of how well the pairwise similarity distribution in high-dimensional space is preserved in the low-dimensional projection, making it a useful tool for benchmarking and diagnosing performance, even for algorithms that do not directly optimize it.
- If the scale of one low-dimensional embedding is significantly smaller or larger than the scale of another low-dimensional embedding, this difference may lead to unreasonably high values of normalized stress, indicating poor preservation of the original high-dimensional data structure.

# Method Summary
- Evaluating with KL divergence provides a measure of how well the pairwise similarity distribution in high-dimensional space is preserved in the low-dimensional projection, making it a useful tool for benchmarking and diagnosing performance, even for algorithms that do not directly optimize it.
- While stress is directly optimized by the MDS family of algorithms, it is a widely used evaluation metric for other DR methods, regardless of whether they optimize it explicitly, implicitly, or not at all.
- Empirical Results: KL Experiment 2A We evaluate the projections with the various KL divergence metrics using the t-SNE, MDS, and RND algorithms.
- Scale-sensitive Divergences We discuss the divergences defined by variants of the tSNE algorithm that are scale-sensitive when used as quality measures. t-SNE KL Divergence (Student‚Äôs t)In t-SNE, the joint probability distributions P={p i j} and Q={q i j} act as similarity measures between points in the dataset and embedding respectively.
- Therefore, in broad strokes, we would only expect to see a minimum at 0 for embeddings generated by extremely poor algorithms, and for algorithms that are actually performant and therefore worthy of comparing through this metric at all, we would see a maximum at 0.
- Nearly all successful DR techniques optimize some function that expresses the quality of the projection, e.g., the Rayleigh quotient for Principal Component Analysis (PCA) [30] or the cross-entropy for Uniform Manifold Approximation and Projection (UMAP) [46].
- We observe that all scale-invariant metrics behave predictably (stress increases when more noise is added) while the scale-sensitive normalized stress is unpredictable and inconsistent. algorithms to seemingly ‚Äúimprove‚Äù as the embedding is perturbed.

# When To Use / Not Use
- Use when:
  - Scale-sensitive Divergences We discuss the divergences defined by variants of the tSNE algorithm that are scale-sensitive when used as quality measures. t-SNE KL Divergence (Student‚Äôs t)In t-SNE, the joint probability distributions P={p i j} and Q={q i j} act as similarity measures between points in the dataset and embedding respectively.
  - NOTE THAT WHILENORMALIZED STRESS(NS)PERFORMS AS EXPECTED AT THE DEFAULT SCALE, INCREASING THE SIZE OF THE EMBEDDING BY10 (CHANGING NOTHING MEANINGFUL ABOUT THE EMBEDDING)CAUSESNSTO SHOW OBVIOUSLY INCORRECT RESULTS IN MOST CASES,ESPECIALLY EXAGGERATED WHEN EVALUATING THERANDOMEMBEDDING TO BE BETTER THANMDSIN 80%OF TRIALS.
  - We observe that all scale-invariant metrics behave predictably (stress increases when more noise is added) while the scale-sensitive normalized stress is unpredictable and inconsistent. algorithms to seemingly ‚Äúimprove‚Äù as the embedding is perturbed.
- Avoid when:
  - This measure ensures that the stress value is more sensitive to smaller discrepancies between the original and reduced distances, which can be crucial when measuring the preservation of the structure of the data during DR.
  - DR Techniques Embedding a sufficiently complex high-dimensional dataset into a lower-dimensional space unavoidably causes distortion, which results in the loss of some information in the dataset [11].
- Failure modes:
  - Limitations:While the experiments described here provide insights into the behavior of different metrics used to evaluate DR techniques, there are many limitations.
  - It is best used as a diagnostic tool for neighbor embedding techniques, and should not be used to compare algorithms with different similarity models without care.

# Metrics Mentioned
- `tnc`: Trustworthiness and Continuity behavior.
- `mrre`: mean relative rank errors across neighborhood scales.
- `nh`: label-based neighborhood hit.
- `nd`: neighbor-shape dissimilarity or neighbor distortion.
- `kl_div`: Kullback-Leibler divergence behavior.
- `dsc`: distance consistency for class separation.
- `pr`: pairwise-distance correlation behavior.
- `srho`: rank-correlation of pairwise distances.
- `snc`: Steadiness/Cohesiveness inter-cluster reliability.
- `topo`: topology-related structure behavior.

# Implementation Notes
- Finally, our experiments do not account for potential variations in the performance of DR techniques due to hyperparameter settings or initialization conditions, which can influence the quality of DR techniques [13].
- We set the perplexity parameter to the value 30 (recommended by van der Maaten and Hinton [73]) for all datasets.
- In particular, our contributions are: ‚Ä¢ Showing analytically and empirically that normalized stress and KL divergence are severely affected by scale and can lead to incorrect conclusions ‚Ä¢ Demonstrating that using a scale-invariant stress measure impacts the results of previous work ‚Ä¢ Proposing scale-invariant options for stress and KL divergence We also provide an interactive webpage that demonstrates the problems discussed here with 6 datasets.
- Additionally, we show additional experimental data from the sensitivity experiment. ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì 17 0 5 10 15 20 25 30 Scale value 1 0 1 2 3 4 5 log(Normalized Stress) t-SNE Minimum MDS Minimum Random Minimum 0 5 10 15 20 25 30 Scale value 0.5 1.0 1.5 2.0 2.5 3.0 3.5KL Divergence MDS Minimum t-SNE Minimum Random Minimum MDS t-SNE Random Fig.
- Thus it is possible to define a new metric, KL‚àû, which has a inverse square kernel in Q instead of Student‚Äôs t kernel (and is guaranteed to be scale-invariant since it is equivalent to choosing a specific scale in the original t-SNE KL divergence): KL‚àû(X,Y) =KL(P,Q (‚àû)) However, in our experiments we find that this metric does not perform well.
- Scale-sensitive Divergences We discuss the divergences defined by variants of the tSNE algorithm that are scale-sensitive when used as quality measures. t-SNE KL Divergence (Student‚Äôs t)In t-SNE, the joint probability distributions P={p i j} and Q={q i j} act as similarity measures between points in the dataset and embedding respectively.
- Keep preprocessing, initialization policy, and random-seed protocol fixed when comparing methods.

# Claim Atoms (For Conflict Resolution)
- CLAIM-PAPER2024ZADUREF04-C1 | stance: support | summary: DR Techniques Embedding a sufficiently complex high-dimensional dataset into a lower-dimensional space unavoidably causes distortion, which results in the loss of some information in the dataset [11]. | evidence_ids: PAPER2024ZADUREF04-E1, PAPER2024ZADUREF04-E2
- CLAIM-PAPER2024ZADUREF04-C2 | stance: support | summary: Evaluating with KL divergence provides a measure of how well the pairwise similarity distribution in high-dimensional space is preserved in the low-dimensional projection, making it a useful tool for benchmarking and diagnosing performance, even for algorithms that do not directly optimize it. | evidence_ids: PAPER2024ZADUREF04-E3, PAPER2024ZADUREF04-E4
- CLAIM-PAPER2024ZADUREF04-C3 | stance: support | summary: Finally, our experiments do not account for potential variations in the performance of DR techniques due to hyperparameter settings or initialization conditions, which can influence the quality of DR techniques [13]. | evidence_ids: PAPER2024ZADUREF04-E5, PAPER2024ZADUREF04-E6
- CLAIM-METRIC-DSC-SOURCE-04 | stance: support | summary: This source includes evidence relevant to `dsc` in dimensionality-reduction reliability evaluation. | evidence_ids: PAPER2024ZADUREF04-E1, PAPER2024ZADUREF04-E2
- CLAIM-METRIC-KL_DIV-SOURCE-04 | stance: support | summary: This source includes evidence relevant to `kl_div` in dimensionality-reduction reliability evaluation. | evidence_ids: PAPER2024ZADUREF04-E1, PAPER2024ZADUREF04-E2
- CLAIM-METRIC-MRRE-SOURCE-04 | stance: support | summary: This source includes evidence relevant to `mrre` in dimensionality-reduction reliability evaluation. | evidence_ids: PAPER2024ZADUREF04-E1, PAPER2024ZADUREF04-E2
- CLAIM-METRIC-NH-SOURCE-04 | stance: support | summary: This source includes evidence relevant to `nh` in dimensionality-reduction reliability evaluation. | evidence_ids: PAPER2024ZADUREF04-E1, PAPER2024ZADUREF04-E2
- CLAIM-METRIC-NM_STRESS-SOURCE-04 | stance: support | summary: This source includes evidence relevant to `nm_stress` in dimensionality-reduction reliability evaluation. | evidence_ids: PAPER2024ZADUREF04-E1, PAPER2024ZADUREF04-E2
- CLAIM-METRIC-PR-SOURCE-04 | stance: support | summary: This source includes evidence relevant to `pr` in dimensionality-reduction reliability evaluation. | evidence_ids: PAPER2024ZADUREF04-E1, PAPER2024ZADUREF04-E2
- CLAIM-METRIC-SN_STRESS-SOURCE-04 | stance: support | summary: This source includes evidence relevant to `sn_stress` in dimensionality-reduction reliability evaluation. | evidence_ids: PAPER2024ZADUREF04-E1, PAPER2024ZADUREF04-E2
- CLAIM-METRIC-SNC-SOURCE-04 | stance: support | summary: This source includes evidence relevant to `snc` in dimensionality-reduction reliability evaluation. | evidence_ids: PAPER2024ZADUREF04-E1, PAPER2024ZADUREF04-E2
- CLAIM-METRIC-SRHO-SOURCE-04 | stance: support | summary: This source includes evidence relevant to `srho` in dimensionality-reduction reliability evaluation. | evidence_ids: PAPER2024ZADUREF04-E1, PAPER2024ZADUREF04-E2
- CLAIM-METRIC-STRESS-SOURCE-04 | stance: support | summary: This source includes evidence relevant to `stress` in dimensionality-reduction reliability evaluation. | evidence_ids: PAPER2024ZADUREF04-E1, PAPER2024ZADUREF04-E2
- CLAIM-METRIC-TNC-SOURCE-04 | stance: support | summary: This source includes evidence relevant to `tnc` in dimensionality-reduction reliability evaluation. | evidence_ids: PAPER2024ZADUREF04-E1, PAPER2024ZADUREF04-E2

# Workflow Relevance Map
- step: 1 | relevance: medium | note: supports task clarification before model selection
- step: 2 | relevance: medium | note: adds constraints for preprocessing and data-audit checks
- step: 3 | relevance: high | note: directly informs task-aligned technique/metric selection
- step: 5 | relevance: medium | note: adds initialization sensitivity guidance
- step: 6 | relevance: high | note: adds hyperparameter sensitivity or optimization guidance

# Evidence
- PAPER2024ZADUREF04-E1 | page: 3, section: extracted, quote: "DR Techniques Embedding a sufficiently complex high-dimensional dataset into a lower-dimensional space unavoidably causes distortion, which results in the loss of some information in the dataset [11]."
- PAPER2024ZADUREF04-E2 | page: 2, section: extracted, quote: "In particular, our contributions are: ‚Ä¢ Showing analytically and empirically that normalized stress and KL divergence are severely affected by scale and can lead to incorrect conclusions ‚Ä¢ Demonstrating that using a scale-invariant stress measure impacts the results of previous work ‚Ä¢ Proposing scale-invariant options for stress and KL divergence We also provide an interactive webpage that demonstrates the problems discussed here with 6 datasets."
- PAPER2024ZADUREF04-E3 | page: 15, section: extracted, quote: "We consider a dataset of 3 samples, whose high-dimensional probability distribution we set as follows: P= Ô£Æ Ô£∞ 0 0.2 0.1 0.2 0 0.2 0.1 0.2 0 Ô£π Ô£ª For this dataset, we create the following embedding: Y=  0 0  ,  0 1  ,  1 1  The following graph for the corresponding KL divergence function clearly depicts KL divergence approaching its horizontal asymptote of 0 from above."
- PAPER2024ZADUREF04-E4 | page: 1, section: extracted, quote: "Evaluating with KL divergence provides a measure of how well the pairwise similarity distribution in high-dimensional space is preserved in the low-dimensional projection, making it a useful tool for benchmarking and diagnosing performance, even for algorithms that do not directly optimize it."
- PAPER2024ZADUREF04-E5 | page: 3, section: extracted, quote: "If the scale of one low-dimensional embedding is significantly smaller or larger than the scale of another low-dimensional embedding, this difference may lead to unreasonably high values of normalized stress, indicating poor preservation of the original high-dimensional data structure."
- PAPER2024ZADUREF04-E6 | page: 5, section: extracted, quote: "The Shepard goodness score is then the Spearman rank correlation of the x and y components of the diagram, i.e., the smallest distance in high-dimensional space should still be the smallest in the projection, and the k smallest distance should still be the k smallest."
- PAPER2024ZADUREF04-E7 | page: 1, section: extracted, quote: "While stress is directly optimized by the MDS family of algorithms, it is a widely used evaluation metric for other DR methods, regardless of whether they optimize it explicitly, implicitly, or not at all."
- PAPER2024ZADUREF04-E8 | page: 11, section: extracted, quote: "Empirical Results: KL Experiment 2A We evaluate the projections with the various KL divergence metrics using the t-SNE, MDS, and RND algorithms."
- PAPER2024ZADUREF04-E9 | page: 6, section: extracted, quote: "Scale-sensitive Divergences We discuss the divergences defined by variants of the tSNE algorithm that are scale-sensitive when used as quality measures. t-SNE KL Divergence (Student‚Äôs t)In t-SNE, the joint probability distributions P={p i j} and Q={q i j} act as similarity measures between points in the dataset and embedding respectively."
- PAPER2024ZADUREF04-E10 | page: 14, section: extracted, quote: "Therefore, in broad strokes, we would only expect to see a minimum at 0 for embeddings generated by extremely poor algorithms, and for algorithms that are actually performant and therefore worthy of comparing through this metric at all, we would see a maximum at 0."
- PAPER2024ZADUREF04-E11 | page: 1, section: extracted, quote: "Nearly all successful DR techniques optimize some function that expresses the quality of the projection, e.g., the Rayleigh quotient for Principal Component Analysis (PCA) [30] or the cross-entropy for Uniform Manifold Approximation and Projection (UMAP) [46]."
- PAPER2024ZADUREF04-E12 | page: 9, section: extracted, quote: "We observe that all scale-invariant metrics behave predictably (stress increases when more noise is added) while the scale-sensitive normalized stress is unpredictable and inconsistent. algorithms to seemingly ‚Äúimprove‚Äù as the embedding is perturbed."
- PAPER2024ZADUREF04-E13 | page: 2, section: extracted, quote: "MDS [64], t-SNE [73], and RND (random) embeddings of the well-known Iris dataset from left to right (bottom)."
- PAPER2024ZADUREF04-E14 | page: 3, section: extracted, quote: "The DR transformation results in a matrix Y‚ààR N√ót called the projection."
