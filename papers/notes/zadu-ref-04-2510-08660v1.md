---
id: paper-2025-zadu-ref-04
title: "How Scale Breaks Normalized Stress and KL Divergence: Rethinking Quality Metrics"
authors: "Kiran Smelser; Kaviru Gunaratne; Jacob Miller; Stephen Kobourov"
venue: "IEEE Transactions on Visualization and Computer Graphics (preprint)"
year: 2025
tags: [dr, zadu-table1-reference, stress, sn_stress, nm_stress, srho, kl_div]
source_pdf: papers/raw/zadu-table1-references/2510.08660v1.pdf
seed_note_id: "paper-2023-zadu-library"
evidence_level: high
updated_at: 2026-02-13
---

# Problem
- This paper extends the scale-sensitivity analysis from stress to KL-divergence-based DR evaluation.
- It shows that both normalized stress and commonly used KL-based quality formulations can change substantially under uniform embedding scaling.
- The practical risk is mis-ranking DR methods and drawing wrong conclusions from apparently objective scalar scores.

# Method Summary
- The paper gives analytical and empirical analyses for stress variants and KL-divergence variants across multiple datasets and DR methods.
- For stress, it studies raw, normalized, non-metric, Shepard, and scale-normalized variants.
- For KL divergence, it analyzes t-SNE-style formulations, shows scale dependence behavior, and proposes scale-aware variants including scale-normalized KL (SNKL).
- It benchmarks ranking consistency and expected-order recovery under controlled experiments, including random baselines.

# When To Use / Not Use
- Use when:
  - you evaluate DR quality with stress or KL objectives across techniques with different output scales,
  - you need method rankings that remain stable under uniform resizing of embeddings.
- Avoid when:
  - KL-based scores are used as universal cross-method quality metrics without checking kernel/similarity-model compatibility.
- Failure modes:
  - scale-sensitive KL/stress can make an inferior embedding appear better at selected scales.

# Metrics Mentioned
- `stress`: analyzed in depth for scale effects and ranking instability.
- `sn_stress`: scale-normalized stress used as a safer alternative.
- `nm_stress`: included as a scale-invariant reference.
- `srho`: Shepard-type rank-correlation perspective is used as a scale-invariant comparator.
- `kl_div`: shown to be scale-sensitive in common formulations; scale-normalized KL variants are proposed.

# Implementation Notes
- If KL divergence is used for evaluation, keep the similarity model consistent across methods; otherwise interpretation degrades.
- Use scale-normalized variants (SNS/SNKL) for cross-method ranking to avoid artificial ordering flips.
- The paper fixes perplexity (`30`) in KL experiments and highlights that hyperparameters/initialization still matter for fair comparison.

# Claim Atoms (For Conflict Resolution)
- CLAIM-ZR04-C1 | stance: support | summary: Normalized stress and KL divergence can both be severely scale-sensitive in DR evaluation. | evidence_ids: ZR04-E1, ZR04-E2, ZR04-E3
- CLAIM-ZR04-C2 | stance: support | summary: Scale-invariant variants materially change benchmark rankings relative to scale-sensitive counterparts. | evidence_ids: ZR04-E4, ZR04-E5, ZR04-E6
- CLAIM-ZR04-C3 | stance: support | summary: KL divergence should be used carefully and not as a universal cross-method comparator without model-awareness. | evidence_ids: ZR04-E7, ZR04-E8

# Workflow Relevance Map
- step: 3 | relevance: high | note: metric selection must distinguish scale-sensitive and scale-invariant quality scores.
- step: 4 | relevance: high | note: technique ranking should rely on scale-invariant scoring to prevent order inversions.
- step: 6 | relevance: medium | note: KL-related hyperparameters (e.g., perplexity) and initialization remain important during optimization.

# Evidence
- ZR04-E1 | page: 1, section: abstract, quote: "normalized stress ... is sensitive to uniform scaling" and "KL divergence ... is also susceptible to this scale sensitivity."
- ZR04-E2 | page: 2, section: contributions, quote: "normalized stress and KL divergence are severely affected by scale and can lead to incorrect conclusions"
- ZR04-E3 | page: 7, section: KL experiment figure text, quote: "KL divergence varies substantially with scale, and the expected order of t-SNE < MDS < Random is not always observed."
- ZR04-E4 | page: 4, section: table I, quote: "Scale-normalized stress ... Yes" in the scale-invariant column.
- ZR04-E5 | page: 7, section: SNKL description, quote: "we ... propose scale-normalized KL divergence: SNKL(X,Y)=min alpha>0 KL(P||Q(alpha))."
- ZR04-E6 | page: 10, section: stress results discussion, quote: "using scale-invariant stress measures impacts the results"
- ZR04-E7 | page: 7, section: KL discussion, quote: "it is not recommended to use KLG as a quality measure unless it is important to have a symmetric divergence"
- ZR04-E8 | page: 12, section: limitations, quote: "it should only be done in select circumstances ... should not be used to compare algorithms with different similarity models without care."
