---
id: paper-2009-zadu-ref-02
title: "Quality assessment of dimensionality reduction: Rank-based criteria"
authors: "John A. Lee, Michel Verleysen"
venue: "Neurocomputing"
year: 2009
tags: [dr, zadu-table1-reference, lcmc, mrre, stress, tnc]
source_pdf: papers/raw/zadu-table1-references/1-s2.0-S0925231209000101-main.pdf
evidence_level: medium
updated_at: 2026-02-08
---
# Problem
- Lee /C3,1,2, Michel Verleysen 3 Machine Learning Group, Universite´ catholique de Louvain, Place du Levant, 3, B-1348 Louvain-la-Neuve, Belgium article info Available online 10 January 2009 Keywords: Dimensionality reduction Embedding Quality assessment Co-ranking matrix Trustworthiness and continuity Intrusion and extrusion fractions abstract Dimensionality reduction aims at providing low-dimensional representations of high-dimensional data sets.
- His research interests include machine learning, artiﬁcial neural networks, self-organization, time-series forecasting, nonlinear statistics, adaptive signal processing, and high-dimensional data analysis.
- If the data set does not specify all distances, then the problem can elegantly be modeled using a graph, in which edges are present for known entries of the pairwise distance matrix.
- Introduction Research about dimensionality reduction (DR) deals with techniques that provide a meaningful low-dimensional representation of a high-dimensional data set.
- In the most general setting, DR transforms a set of N high-dimensional vectors, denoted N ¼½ ni/C1381pipN , into N lowdimensional vectors, denoted X ¼½ xi/C1381pipN .

# Method Summary
- As most NLDR methods optimize a given objective function, a simplistic way to assess the quality is to look at the value of the objective function after ARTICLE IN PRESS Contents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/neucom Neurocomputing 0925-2312/$ - see front matter & 2009 Elsevier B.V.
- The considered NLDR methods are PCA [13], Isomap [34], Sammon’s NLM [29] (with both Euclidean and geodesic distances [19,21], NLM and GNLM), CCA [8], cuvilinear distance analysis (CDA) [19], Kohonen’s SOM [15], locally linear embedding (LLE) [27], Laplacian eigenmaps (LE) [2,3] , and Isotop [18].
- However, as the objective function is usually intended to be optimized by typical techniques such as gradient descent, it must fulﬁll some requirements as to continuity and differentiability.
- The considered NLDR methods are PCA, Isomap, Sammon’s NLM (with Euclidean and geodesic distances), CCA, CDA, Kohonen’s SOM, locally linear embedding, Laplacian eigenmaps, and Isotop.
- As to its sign, experiments show that methods that can produce extrusive embeddings usually attain higher values of the quality criterion.
- Kaski, Neighborhood preservation in nonlinear projection methods: an experimental study, in: G.
- Lee /C3,1,2, Michel Verleysen 3 Machine Learning Group, Universite´ catholique de Louvain, Place du Levant, 3, B-1348 Louvain-la-Neuve, Belgium article info Available online 10 January 2009 Keywords: Dimensionality reduction Embedding Quality assessment Co-ranking matrix Trustworthiness and continuity Intrusion and extrusion fractions abstract Dimensionality reduction aims at providing low-dimensional representations of high-dimensional data sets.

# When To Use / Not Use
- Use when:
  - He is Editor-in-Chief of the Neural Processing Letters journal, Chairman of the Annual European Symposium on Artiﬁcial Neural Networks (ESANN) Conference, Associate Editor of the IEEE Transactions on Neural Networks journal, and member of the editorial board and program committee of several journals and conferences on neural networks and learning.
  - Historically, this scatterplot has often been used to assess results of multidimensional scaling and related methods [8];i t shows the distances dij with respect to the corresponding distances dij, for all pairs ði; jÞ, with iaj.
  - Frey (Eds.), Proceedings of International Workshop on Artiﬁcial Intelligence and Statistics (AISTATS’03), Key West, FL, 2003, also presented at NIPS 2002 Workshop on Spectral Methods, Available as Technical Report TR2002-042.
- Avoid when:
  - In each ﬁgure, the left diagram shows the whole curves, for 1 pKpN /C0 1; the upper right diagram focuses on the quality criterion for small values of K, whereas the last one does the same for the behavior indicator.
  - For larger values, we can see that the common weighting shared by the MRREs and fW1;1 N ðKÞ; W1;1 X ðKÞg gives a higher importance to local errors; as a consequence, the curves essentially remain ﬂat when K grows.
- Failure modes:
  - In fact, any weighting in the quality measures inevitably turns out to be arbitrary: weighted averages tend to purposely emphasize some types of embedding errors but therefore they can also fail to detect others.
  - His research interests include machine learning, artiﬁcial neural networks, self-organization, time-series forecasting, nonlinear statistics, adaptive signal processing, and high-dimensional data analysis.

# Metrics Mentioned
- `tnc`: Trustworthiness and Continuity behavior.
- `mrre`: mean relative rank errors across neighborhood scales.
- `lcmc`: local continuity meta-criterion for neighborhood overlap.
- `nh`: label-based neighborhood hit.
- `nd`: neighbor-shape dissimilarity or neighbor distortion.
- `pr`: pairwise-distance correlation behavior.
- `topo`: topology-related structure behavior.
- `proc`: Procrustes local shape agreement.
- `stress`: stress-based distance distortion.
- `qnx`: QNX neighborhood-rank quality.

# Implementation Notes
- Considering that K is a sort of scale parameter, one can interpret the ﬁgures as follows.
- Unreported experiments on the same data sets allow us to brieﬂy describe the inﬂuence of parameters v and w in Q v;w wNXðKÞ and Bv;w wNXðKÞ as follows.
- Obviously, this allows us to compare several runs with e.g. different parameter values, but makes the comparison of different methods unfair.
- With an appropriate gray scale, the co-ranking matrix can also be displayed and interpreted in a similar way as a Shepard diagram [32].
- (2) Computing Q requires 2 N sorting operations and therefore the time complexity is OðN2 log NÞ with a typical sorting algorithm.
- This is expected and corresponds to noise ﬂattening on small scales.
- Keep preprocessing, initialization policy, and random-seed protocol fixed when comparing methods.

# Claim Atoms (For Conflict Resolution)
- CLAIM-PAPER2009ZADUREF02-C1 | stance: support | summary: Lee /C3,1,2, Michel Verleysen 3 Machine Learning Group, Universite´ catholique de Louvain, Place du Levant, 3, B-1348 Louvain-la-Neuve, Belgium article info Available online 10 January 2009 Keywords: Dimensionality reduction Embedding Quality assessment Co-ranking matrix Trustworthiness and continuity Intrusion and extrusion fractions abstract Dimensionality reduction aims at providing low-dimensional representations of high-dimensional data sets. | evidence_ids: PAPER2009ZADUREF02-E1, PAPER2009ZADUREF02-E2
- CLAIM-PAPER2009ZADUREF02-C2 | stance: support | summary: As most NLDR methods optimize a given objective function, a simplistic way to assess the quality is to look at the value of the objective function after ARTICLE IN PRESS Contents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/neucom Neurocomputing 0925-2312/$ - see front matter & 2009 Elsevier B.V. | evidence_ids: PAPER2009ZADUREF02-E3, PAPER2009ZADUREF02-E4
- CLAIM-PAPER2009ZADUREF02-C3 | stance: support | summary: Considering that K is a sort of scale parameter, one can interpret the ﬁgures as follows. | evidence_ids: PAPER2009ZADUREF02-E5, PAPER2009ZADUREF02-E6
- CLAIM-METRIC-LCMC-SOURCE-02 | stance: support | summary: This source includes evidence relevant to `lcmc` in dimensionality-reduction reliability evaluation. | evidence_ids: PAPER2009ZADUREF02-E1, PAPER2009ZADUREF02-E2
- CLAIM-METRIC-MRRE-SOURCE-02 | stance: support | summary: This source includes evidence relevant to `mrre` in dimensionality-reduction reliability evaluation. | evidence_ids: PAPER2009ZADUREF02-E1, PAPER2009ZADUREF02-E2
- CLAIM-METRIC-STRESS-SOURCE-02 | stance: support | summary: This source includes evidence relevant to `stress` in dimensionality-reduction reliability evaluation. | evidence_ids: PAPER2009ZADUREF02-E1, PAPER2009ZADUREF02-E2
- CLAIM-METRIC-TNC-SOURCE-02 | stance: support | summary: This source includes evidence relevant to `tnc` in dimensionality-reduction reliability evaluation. | evidence_ids: PAPER2009ZADUREF02-E1, PAPER2009ZADUREF02-E2

# Workflow Relevance Map
- step: 1 | relevance: medium | note: supports task clarification before model selection
- step: 2 | relevance: medium | note: adds constraints for preprocessing and data-audit checks
- step: 3 | relevance: high | note: directly informs task-aligned technique/metric selection
- step: 7 | relevance: high | note: supports visualization interpretation and user explanation

# Evidence
- PAPER2009ZADUREF02-E1 | page: 1, section: extracted, quote: "Lee /C3,1,2, Michel Verleysen 3 Machine Learning Group, Universite´ catholique de Louvain, Place du Levant, 3, B-1348 Louvain-la-Neuve, Belgium article info Available online 10 January 2009 Keywords: Dimensionality reduction Embedding Quality assessment Co-ranking matrix Trustworthiness and continuity Intrusion and extrusion fractions abstract Dimensionality reduction aims at providing low-dimensional representations of high-dimensional data sets."
- PAPER2009ZADUREF02-E2 | page: 13, section: extracted, quote: "His research interests include machine learning, artiﬁcial neural networks, self-organization, time-series forecasting, nonlinear statistics, adaptive signal processing, and high-dimensional data analysis."
- PAPER2009ZADUREF02-E3 | page: 1, section: extracted, quote: "If the data set does not specify all distances, then the problem can elegantly be modeled using a graph, in which edges are present for known entries of the pairwise distance matrix."
- PAPER2009ZADUREF02-E4 | page: 1, section: extracted, quote: "Introduction Research about dimensionality reduction (DR) deals with techniques that provide a meaningful low-dimensional representation of a high-dimensional data set."
- PAPER2009ZADUREF02-E5 | page: 1, section: extracted, quote: "In the most general setting, DR transforms a set of N high-dimensional vectors, denoted N ¼½ ni/C1381pipN , into N lowdimensional vectors, denoted X ¼½ xi/C1381pipN ."
- PAPER2009ZADUREF02-E6 | page: 2, section: extracted, quote: "The rank of nj with respect to ni in the high-dimensional space is written as rij ¼j fk : dikodij or ðdik ¼ dij and kojÞgj, where j/C1j denotes the set cardinality."
- PAPER2009ZADUREF02-E7 | page: 1, section: extracted, quote: "As most NLDR methods optimize a given objective function, a simplistic way to assess the quality is to look at the value of the objective function after ARTICLE IN PRESS Contents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/neucom Neurocomputing 0925-2312/$ - see front matter & 2009 Elsevier B.V."
- PAPER2009ZADUREF02-E8 | page: 10, section: extracted, quote: "The considered NLDR methods are PCA [13], Isomap [34], Sammon’s NLM [29] (with both Euclidean and geodesic distances [19,21], NLM and GNLM), CCA [8], cuvilinear distance analysis (CDA) [19], Kohonen’s SOM [15], locally linear embedding (LLE) [27], Laplacian eigenmaps (LE) [2,3] , and Isotop [18]."
- PAPER2009ZADUREF02-E9 | page: 2, section: extracted, quote: "However, as the objective function is usually intended to be optimized by typical techniques such as gradient descent, it must fulﬁll some requirements as to continuity and differentiability."
- PAPER2009ZADUREF02-E10 | page: 9, section: extracted, quote: "The considered NLDR methods are PCA, Isomap, Sammon’s NLM (with Euclidean and geodesic distances), CCA, CDA, Kohonen’s SOM, locally linear embedding, Laplacian eigenmaps, and Isotop."
- PAPER2009ZADUREF02-E11 | page: 8, section: extracted, quote: "As to its sign, experiments show that methods that can produce extrusive embeddings usually attain higher values of the quality criterion."
- PAPER2009ZADUREF02-E12 | page: 13, section: extracted, quote: "Kaski, Neighborhood preservation in nonlinear projection methods: an experimental study, in: G."
- PAPER2009ZADUREF02-E13 | page: 2, section: extracted, quote: "Obviously, this allows us to compare several runs with e.g. different parameter values, but makes the comparison of different methods unfair."
- PAPER2009ZADUREF02-E14 | page: 3, section: extracted, quote: "Notice that the embedding quality is described by two criteria, which distinguish two types of errors."
