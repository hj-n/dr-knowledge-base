---
id: paper-2016-pending-extra-1-s2-0-s147692711630
title: "Comparison among dimensionality reduction techniques based on Random Projection for cancer classification"
authors: "Haozhe Xie, Jie Li, Qiaosheng Zhang, Yadong Wang"
venue: "Computational Biology and Chemistry"
year: 2016
tags: [dr, reliability, visual_analytics, reference, pending_references]
source_pdf: papers/raw/pending-references/1-s2.0-S1476927116304108-main.pdf
seed_note_id: "paper-2025-jeon-reliable-va-survey"
evidence_level: medium
updated_at: 2026-02-08
---

# Problem
- To address this problem, Random Projection (RP) was purposed inDasgupta (2000), which projects original data onto a low-dimensional subspace randomly that the discriminative information can be approximately retained, and have been developing rapidly these years ( Zhao and Mao, 2015; Zhao et al., 2016; Geppert et al., 2015).
- The optimization problem for PCA can be summarized as follows: W ∗ = argmaxW ∈ Rd×k ⏐⏐ ⏐ ⏐ WT SW WT W ⏐ ⏐ ⏐ ⏐ (2) where S can be deﬁned as follows: S = 1 n n∑ i=1 (xi − ¯x)(xi − ¯x)T (3) where xi is a vector of d dimensions and ¯x represents the mean vector of all samples.
- abstract Random Projection (RP) technique has been widely applied in many scenarios because it can reduce high-dimensional features into low-dimensional space within short time and meet the need of real-time analysis of massive data.
- The optimization problem for LDA is described as follows: W ∗ = argmaxW ∈ Rd×k ⏐⏐ ⏐ ⏐ WT SBW WT SW W ⏐ ⏐ ⏐ ⏐ (4) where SB and SW are given as follows: ⎧ ⎪⎪ ⎪ ⎪ ⎨ ⎪⎪ ⎪ ⎪ ⎩ SB = Nc∑ c=1 nc(¯xc − ¯x)(¯xc − ¯x)T SW = n∑ i=1 (xi − ¯

# Method Summary
- Feature Selection + Random Projection + Post Feature Selection (FS + RP + PFS) In the method, we plan to employ a new method called “Post Feature Selection” (PFS) to select features after FS + RP in order to ﬁnd a set of features which have sufﬁcient discriminative power.
- Feature Selection + Random Projection (FS + RP) In this method, FS is used before RP to ﬁlter dimensions that are similar between two classes of samples in original dataset.
- Feature Selection + Random Projection + Linear Discriminant Analysis (FS + RP + LDA) Here three methods: FS, RP and LDA are combined to reduce dimensionality.
- Combination methods based on Random Projection In this section, RP is combined with other methods, such as PCA, LDA and FS to improve the performance of RP.
- Veriﬁcation To further validate the performance of different methods, we repeat the above experiments on a simulation dataset: SData, which includes 100 positive samples and 100 negative samples with 10,000 features, and each feature in SData follows normal distributions: N(0, 0.1) and N(0 ± r, 0.1) for positive and negative samples respectively, herer ∈ [−0.125, 0.125].
- Datasets and experimental environment In this section, the performance of methods described above was tested and compared on three breast cancer gene expression data (Network et al., 2012; Wang et al., 2005; Hatzis et al., 2011 ), which can be downloaded from http://tcga-data.nci.nih.gov/tcga and http://www.ncbi.nlm.nih.gov/geo.
- To address this problem, Random Projection (RP) was purposed inDasgupta (2000), which projects original data onto a low-dimensional subspace randomly that the discriminative information can be approximately retained, and have been developing rapidly these years ( Zhao and Mao, 2015; Zhao et al., 2016; Geppert et al., 2015).

# When To Use / Not Use
- Use when:
  - On all datasets, classiﬁcation accuracy of SVM based on RP is lower than that of RP + FS, because RP projects original data to subspace where samples cannot be effectively divided into two groups in some dimensions (named “invalid dimensions”) and FS ﬁlters out features from invalid dimensions after RP.
  - However, RP may not capture task-related information because latent space is generated without considering the structure of original data, and accuracy of http://dx.doi.org/10.1016/j.compbiolchem.2016.09.010 1476-9271/© 2016 Elsevier Ltd.
- Avoid when:
  - Wrapper method for feature selection Kohavi and John (1997) trains a new model for classiﬁcation for each subset, which can usually provide the best performing set for that particular type of model.
  - In spite of this fact, direct use of PCA, LDA and FS may be problematic because these methods require impracticably large computational resources and cannot meet the needs of real-time processing.
- Failure modes:
  - A Support Vector Machine (SVM) with radial basis kernel function is used to evaluate the classiﬁcation accuracy for each dimension, where top k dimensions with higher accuracy are selected.

# Metrics Mentioned
- `rank-based quality criteria`: referenced as part of projection-quality or reliability assessment.

# Implementation Notes
- The optimization problem for PCA can be summarized as follows: W ∗ = argmaxW ∈ Rd×k ⏐⏐ ⏐ ⏐ WT SW WT W ⏐ ⏐ ⏐ ⏐ (2) where S can be deﬁned as follows: S = 1 n n∑ i=1 (xi − ¯x)(xi − ¯x)T (3) where xi is a vector of d dimensions and ¯x represents the mean vector of all samples.
- Feature Selection + Random Projection (FS + RP) In this method, FS is used before RP to ﬁlter dimensions that are similar between two classes of samples in original dataset.
- Feature Selection + Random Projection + Linear Discriminant Analysis (FS + RP + LDA) Here three methods: FS, RP and LDA are combined to reduce dimensionality.
- Combination methods based on Random Projection In this section, RP is combined with other methods, such as PCA, LDA and FS to improve the performance of RP.
- Veriﬁcation To further validate the performance of different methods, we repeat the above experiments on a simulation dataset: SData, which includes 100 positive samples and 100 negative samples with 10,000 features, and each feature in SData follows normal distributions: N(0, 0.1) and N(0 ± r, 0.1) for positive and negative samples respectively, herer ∈ [−0.125, 0.125].
- Keep preprocessing, initialization policy, and seed protocol fixed when comparing methods or parameter settings.

# Claim Atoms (For Conflict Resolution)
- CLAIM-PENDING-EXTRA-C1 | stance: support | summary: To address this problem, Random Projection (RP) was purposed inDasgupta (2000), which projects original data onto a low-dimensional subspace randomly that the discriminative information can be approximately retained, and have been developing rapidly these years ( Zhao and Mao, 2015; Zhao et al., 2016; Geppert et al., 2015). | evidence_ids: PENDING-EXTRA-E1, PENDING-EXTRA-E2
- CLAIM-PENDING-EXTRA-C2 | stance: support | summary: Feature Selection + Random Projection + Post Feature Selection (FS + RP + PFS) In the method, we plan to employ a new method called “Post Feature Selection” (PFS) to select features after FS + RP in order to ﬁnd a set of features which have sufﬁcient discriminative power. | evidence_ids: PENDING-EXTRA-E3, PENDING-EXTRA-E4
- CLAIM-PENDING-EXTRA-C3 | stance: support | summary: The optimization problem for PCA can be summarized as follows: W ∗ = argmaxW ∈ Rd×k ⏐⏐ ⏐ ⏐ WT SW WT W ⏐ ⏐ ⏐ ⏐ (2) where S can be deﬁned as follows: S = 1 n n∑ i=1 (xi − ¯x)(xi − ¯x)T (3) where xi is a vector of d dimensions and ¯x represents the mean vector of all samples. | evidence_ids: PENDING-EXTRA-E5, PENDING-EXTRA-E6

# Workflow Relevance Map
- step: 1 | relevance: medium | note: supports explicit task clarification before DR recommendation
- step: 2 | relevance: medium | note: adds preprocessing/data-condition constraints for reliable comparison
- step: 3 | relevance: high | note: directly informs task-aligned method and metric selection
- step: 5 | relevance: high | note: provides hyperparameter sensitivity/optimization guidance

# Evidence
- PENDING-EXTRA-E1 | page: 1, section: extracted, quote: "To address this problem, Random Projection (RP) was purposed inDasgupta (2000), which projects original data onto a low-dimensional subspace randomly that the discriminative information can be approximately retained, and have been developing rapidly these years ( Zhao and Mao, 2015; Zhao et al., 2016; Geppert et al., 2015)."
- PENDING-EXTRA-E2 | page: 2, section: extracted, quote: "The optimization problem for PCA can be summarized as follows: W ∗ = argmaxW ∈ Rd×k ⏐⏐ ⏐ ⏐ WT SW WT W ⏐ ⏐ ⏐ ⏐ (2) where S can be deﬁned as follows: S = 1 n n∑ i=1 (xi − ¯x)(xi − ¯x)T (3) where xi is a vector of d dimensions and ¯x represents the mean vector of all samples."
- PENDING-EXTRA-E3 | page: 1, section: extracted, quote: "abstract Random Projection (RP) technique has been widely applied in many scenarios because it can reduce high-dimensional features into low-dimensional space within short time and meet the need of real-time analysis of massive data."
- PENDING-EXTRA-E4 | page: 2, section: extracted, quote: "The optimization problem for LDA is described as follows: W ∗ = argmaxW ∈ Rd×k ⏐⏐ ⏐ ⏐ WT SBW WT SW W ⏐ ⏐ ⏐ ⏐ (4) where SB and SW are given as follows: ⎧ ⎪⎪ ⎪ ⎪ ⎨ ⎪⎪ ⎪ ⎪ ⎩ SB = Nc∑ c=1 nc(¯xc − ¯x)(¯xc − ¯x)T SW = n∑ i=1 (xi − ¯"
- PENDING-EXTRA-E5 | page: 3, section: extracted, quote: "Feature Selection + Random Projection + Post Feature Selection (FS + RP + PFS) In the method, we plan to employ a new method called “Post Feature Selection” (PFS) to select features after FS + RP in order to ﬁnd a set of features which have sufﬁcient discriminative power."
- PENDING-EXTRA-E6 | page: 3, section: extracted, quote: "Feature Selection + Random Projection (FS + RP) In this method, FS is used before RP to ﬁlter dimensions that are similar between two classes of samples in original dataset."
- PENDING-EXTRA-E7 | page: 3, section: extracted, quote: "Feature Selection + Random Projection + Linear Discriminant Analysis (FS + RP + LDA) Here three methods: FS, RP and LDA are combined to reduce dimensionality."
- PENDING-EXTRA-E8 | page: 3, section: extracted, quote: "Combination methods based on Random Projection In this section, RP is combined with other methods, such as PCA, LDA and FS to improve the performance of RP."
