---
id: paper-2011-pending-extra-1-s2-0-s187705091100
title: "Shift-invariant similarities circumvent distance concentration in stochastic neighbor embedding and variants"
authors: "John A. Lee, Michel Verleysen"
venue: "Procedia Computer Science"
year: 2011
tags: [dr, reliability, visual_analytics, reference, pending_references]
source_pdf: papers/raw/pending-references/1-s2.0-S1877050911001141-main.pdf
seed_note_id: "paper-2025-jeon-reliable-va-survey"
evidence_level: medium
updated_at: 2026-02-08
---

# Problem
- Lee a,1, Michel V erleysenb aDepartment of Molecular Imaging, Radiotherapy, and Oncology, Universit´ e catholique de Louvain, Brussels (Belgium) bMachine Learning Group, ICTEAM institute Universit´ e catholique de Louvain, Louvain-la-Neuve (Belgium) Abstract Dimensionality reduction aims at representing high-dimensional data in low-dimensional spaces, mainly for visualization and exploratory purposes.
- The update for the gradient descent becomes x i←xi+α∑ j(σij−sij+σji−s ji)(xi−x j)/(1+ d2 ij ) The discrepancy between the Gaussian similarities in the high-dimensional space and the heavy-tailed ones in the low-dimensional space amounts to applying an exponential transformation to δ ij to obtain dij [26].
- Stretched distances are assumed to circumvent a so-called ‘crowding problem’ [21], which intuitively refers to fact NLDR requires data distributed in vast (hyper-)volumes to be ‘packed’ and displayed on a limited surface in the low-dimensional space.
- Such a shift invariance facilitates the NLDR process by circumventing the fact that the phenomenon of norm concentration [25] manifests itself diﬀerently in the high-dimensional data space and the low-dimensional visualization space.

# Method Summary
- Recent and successful methods following this approach are stochastic neighbor embedding (SNE) [20] and its variants, t-distributed SNE (t-SNE) [21] and NeRV (standing for neighborhood retrieval and visualization) [22].
- V ery recently, similarity preservation emerged as a new paradigm for dimensionality reduction, with methods such as stochastic neighbor embedding and its variants.
- In t-SNE, the modiﬁcations concern mainly the similarities in the low-dimensional space, which are deﬁned as sij = (1+ d2 ij )−1 ∑ k,l,k/nequall(1+ d2 kl)−1. (4) The name of the method stems from the replacement of the Gaussian shape in (1) with an expression that is closely related to the probability density function of a Student t distribution with a single degree of freedom.
- Available online at www.sciencedirect.com 1877–0509 © 2011 Published by Elsevier Ltd. doi:10.1016/j.procs.2011.04.056 Procedia Computer Science 4 (2011) 538–547 International Conference on Computational Science, ICCS 2011 Shift-invariant similarities circumvent distance concentration in stochastic neighbor embedding and variants John A.
- In order to address this consistency issue, we propose to modify and rewrite the deﬁnitions of the similarities in the high- and low-dimensional spaces as σ ij = exp(−max(δij,τi)2/(2λ2 i )) ∑ k exp(−max(δik,τi)2/(2λ2 i )) and sij = exp(−max(dij, ti)2/2)∑ k exp(−max(dik, ti)2/2) , (7) whereτi and ti are thresholds.
- These contradictory results motivate a thorough investigation in order to clearly elucidate why similarity preservation can be so successful in SNE, t-SNE and NeRV , and to exploit the successful features of these methods in further developments of the ﬁeld.
- As an alternative to projections on linear subspaces, nonlinear dimensionality reduction, also known as manifold learning, can provide data representations that preserve structural properties such as pairwise distances or local neighborhoods.

# When To Use / Not Use
- Use when:
  - Any limitation of the shift caused by these distances degrades the embedding quality.
  - In this example, the distribution of pairwise distances consists of three components: reﬂexive distances, spurious small distances caused by noise, and eventually large distances related to the video content.
- Avoid when:
  - The identiﬁcation of shift-invariance as a key property in the success of similarity-based NLDR will help us to design better visualization methods, with more eﬀective cost functions and improved robustness.
  - In contrast, the left part of the similarity as used in SNE totally diﬀers but it is unlikely to be exploited as the probability to observe a distance in this region is very low in practice.
- Failure modes:
  - Introduction The interpretation of high-dimensional data remains a diﬃcult task, mainly because human vision cannot deal with spaces having more than three dimensions.

# Metrics Mentioned
- `kl divergence`: referenced as part of projection-quality or reliability assessment.
- `neighborhood-preservation criteria`: referenced as part of projection-quality or reliability assessment.
- `rank-based quality criteria`: referenced as part of projection-quality or reliability assessment.

# Implementation Notes
- The reason is that the CCDF depends on several parameters, such as the number of degrees of freedom and the scaling, whose value is not known and diﬃcult to estimate.
- In particular, it introduces a parameter that controls the maximal shift amplitude and widens the applicability of similarity-based NLDR.
- In SNE, the scaling parameters λi are adjusted in order to equalize all entropies, namely, H= ∑ jσij log(σij ) for all i.
- All visualizations are computed with the same cost function as in t-SNE (perplexity set to 50, 300 iterations).
- However, it is computationally much simpler and its parameterization is not as intricate as in the CCDF.
- Keep preprocessing, initialization policy, and seed protocol fixed when comparing methods or parameter settings.

# Claim Atoms (For Conflict Resolution)
- CLAIM-PENDING-EXTRA-C1 | stance: support | summary: Lee a,1, Michel V erleysenb aDepartment of Molecular Imaging, Radiotherapy, and Oncology, Universit´ e catholique de Louvain, Brussels (Belgium) bMachine Learning Group, ICTEAM institute Universit´ e catholique de Louvain, Louvain-la-Neuve (Belgium) Abstract Dimensionality reduction aims at representing high-dimensional data in low-dimensional spaces, mainly for visualization and exploratory purposes. | evidence_ids: PENDING-EXTRA-E1, PENDING-EXTRA-E2
- CLAIM-PENDING-EXTRA-C2 | stance: support | summary: Recent and successful methods following this approach are stochastic neighbor embedding (SNE) [20] and its variants, t-distributed SNE (t-SNE) [21] and NeRV (standing for neighborhood retrieval and visualization) [22]. | evidence_ids: PENDING-EXTRA-E3, PENDING-EXTRA-E4
- CLAIM-PENDING-EXTRA-C3 | stance: support | summary: The reason is that the CCDF depends on several parameters, such as the number of degrees of freedom and the scaling, whose value is not known and diﬃcult to estimate. | evidence_ids: PENDING-EXTRA-E5, PENDING-EXTRA-E6

# Workflow Relevance Map
- step: 1 | relevance: medium | note: supports explicit task clarification before DR recommendation
- step: 2 | relevance: medium | note: adds preprocessing/data-condition constraints for reliable comparison
- step: 3 | relevance: high | note: directly informs task-aligned method and metric selection
- step: 5 | relevance: high | note: provides hyperparameter sensitivity/optimization guidance

# Evidence
- PENDING-EXTRA-E1 | page: 1, section: extracted, quote: "Lee a,1, Michel V erleysenb aDepartment of Molecular Imaging, Radiotherapy, and Oncology, Universit´ e catholique de Louvain, Brussels (Belgium) bMachine Learning Group, ICTEAM institute Universit´ e catholique de Louvain, Louvain-la-Neuve (Belgium) Abstract Dimensionality reduction aims at representing high-dimensional data in low-dimensional spaces, mainly for visualization and exploratory purposes."
- PENDING-EXTRA-E2 | page: 3, section: extracted, quote: "The update for the gradient descent becomes x i←xi+α∑ j(σij−sij+σji−s ji)(xi−x j)/(1+ d2 ij ) The discrepancy between the Gaussian similarities in the high-dimensional space and the heavy-tailed ones in the low-dimensional space amounts to applying an exponential transformation to δ ij to obtain dij [26]."
- PENDING-EXTRA-E3 | page: 3, section: extracted, quote: "Stretched distances are assumed to circumvent a so-called ‘crowding problem’ [21], which intuitively refers to fact NLDR requires data distributed in vast (hyper-)volumes to be ‘packed’ and displayed on a limited surface in the low-dimensional space."
- PENDING-EXTRA-E4 | page: 2, section: extracted, quote: "Such a shift invariance facilitates the NLDR process by circumventing the fact that the phenomenon of norm concentration [25] manifests itself diﬀerently in the high-dimensional data space and the low-dimensional visualization space."
- PENDING-EXTRA-E5 | page: 2, section: extracted, quote: "Recent and successful methods following this approach are stochastic neighbor embedding (SNE) [20] and its variants, t-distributed SNE (t-SNE) [21] and NeRV (standing for neighborhood retrieval and visualization) [22]."
- PENDING-EXTRA-E6 | page: 1, section: extracted, quote: "V ery recently, similarity preservation emerged as a new paradigm for dimensionality reduction, with methods such as stochastic neighbor embedding and its variants."
- PENDING-EXTRA-E7 | page: 3, section: extracted, quote: "In t-SNE, the modiﬁcations concern mainly the similarities in the low-dimensional space, which are deﬁned as sij = (1+ d2 ij )−1 ∑ k,l,k/nequall(1+ d2 kl)−1. (4) The name of the method stems from the replacement of the Gaussian shape in (1) with an expression that is closely related to the probability density function of a Student t distribution with a single degree of freedom."
- PENDING-EXTRA-E8 | page: 1, section: extracted, quote: "Available online at www.sciencedirect.com 1877–0509 © 2011 Published by Elsevier Ltd. doi:10.1016/j.procs.2011.04.056 Procedia Computer Science 4 (2011) 538–547 International Conference on Computational Science, ICCS 2011 Shift-invariant similarities circumvent distance concentration in stochastic neighbor embedding and variants John A."
