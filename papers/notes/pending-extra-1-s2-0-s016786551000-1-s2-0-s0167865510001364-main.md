---
id: paper-2010-pending-extra-1-s2-0-s016786551000
title: "Scale-independent quality criteria for dimensionality reduction"
authors: "John A. Lee, Michel Verleysen"
venue: "Pattern Recognition Letters"
year: 2010
tags: [dr, reliability, visual_analytics, reference, pending_references]
source_pdf: papers/raw/pending-references/1-s2.0-S0167865510001364-main.pdf
seed_note_id: "paper-2025-jeon-reliable-va-survey"
evidence_level: medium
updated_at: 2026-02-08
---

# Problem
- Introduction The interpretation of high-dimensional data remains a difﬁcult task, mainly because human vision is not used to deal with spaces whose dimensionality is higher than three.
- If not all distances are speciﬁed, then the problem can elegantly be modeled using a graph, in which edges are present for known entries of the pairwise distance matrix.
- If visualization is difﬁcult in high-dimensional space, perhaps an (almost) equivalent representation in a lowerdimensional space could improve the readability of data.
- abstract Dimensionality reduction aims at representing high-dimensional data in low-dimensional spaces, in order to facilitate their visual interpretation.

# Method Summary
- As most NLDR methods optimize a given objective function, a simplistic way to assess quality is to look at the value of the objective function after convergence.
- For the past 25 years, research around NLDR has deeply evolved and after some interest in neural approaches ( Kohonen, 1982; Kramer, 1991; Oja, 1991; Demartines and Hérault, 1993; Mao et al., 1995 ), the community has recently focused on spectral techniques ( Schölkopf et al., 1998; Tenenbaum et al., 2000; Roweis and Saul, 2000; Belkin and Niyogi, 2003; Donoho and Grimes, 2003; Weinberger and Saul, 2006 ).
- Modern NLDR is sometimes referred to as manifold learning; it is also tightly connected with graph embedding ( Di Battista et al., 1999 ) and spectral clustering ( Bengio et al., 2003; Saerens et al., 2004; Nadler et al., 2006; Brand and Huang, 2003 ).
- The reconstruction error is a universal quality criterion, but it requires the availability of M andM/C0 1 in closed form, whereas most NLDR methods are nonparametric (they merely provide values ofM for the known vectors ni).
- Still another approach mentioned in the literature consists in using an indirect performance index, such as a classiﬁcation error (see for instance ( Saul et al., 2003; Weinberger et al., 2004 ) and other
- As these properties cannot easily be identiﬁed starting from a set of Cartesian coordinates, the above-mentioned approaches based on distances, neighborhoods, etc. are followed as well.
- Still, objective functions that assess the preservation of pairwise distances, such as the stress or strain used in various versions of MDS, have been very popular ( Venna, 2007).

# When To Use / Not Use
- Use when:
  - For the past 25 years, research around NLDR has deeply evolved and after some interest in neural approaches ( Kohonen, 1982; Kramer, 1991; Oja, 1991; Demartines and Hérault, 1993; Mao et al., 1995 ), the community has recently focused on spectral techniques ( Schölkopf et al., 1998; Tenenbaum et al., 2000; Roweis and Saul, 2000; Belkin and Niyogi, 2003; Donoho and Grimes, 2003; Weinberger and Saul, 2006 ).
  - For instance, a graph can be used to focus on small neighborhoods (Roweis and Saul, 2000 ) or to approximate geodesic distances (Tenenbaum et al., 2000; Lee and Verleysen, 2004 ) with weighted 0167-8655/$ - see front matter /C2112010 Elsevier B.V.
- Avoid when:
  - Linear DR is well known, with techniques such as principal component analysis ( Jolliffe, 1986 ) and classical metric multidimensional scaling ( Young and Householder, 1938; Torgerson, 1952).
  - Introduction The interpretation of high-dimensional data remains a difﬁcult task, mainly because human vision is not used to deal with spaces whose dimensionality is higher than three.
- Failure modes:
  - Still, objective functions that assess the preservation of pairwise distances, such as the stress or strain used in various versions of MDS, have been very popular ( Venna, 2007).

# Metrics Mentioned
- `trustworthiness`: referenced as part of projection-quality or reliability assessment.
- `continuity`: referenced as part of projection-quality or reliability assessment.
- `stress`: referenced as part of projection-quality or reliability assessment.
- `co-ranking`: referenced as part of projection-quality or reliability assessment.
- `neighborhood-preservation criteria`: referenced as part of projection-quality or reliability assessment.
- `rank-based quality criteria`: referenced as part of projection-quality or reliability assessment.

# Implementation Notes
- Obviously, this allows us to compare several runs with e.g. different parameter values, but makes the comparison of different methods unfair.
- Most dimensionality reduction techniques indeed rely on a scale parameter that distinguish between local and global data properties.
- For the past 25 years, research around NLDR has deeply evolved and after some interest in neural approaches ( Kohonen, 1982; Kramer, 1991; Oja, 1991; Demartines and Hérault, 1993; Mao et al., 1995 ), the community has recently focused on spectral techniques ( Schölkopf et al., 1998; Tenenbaum et al., 2000; Roweis and Saul, 2000; Belkin and Niyogi, 2003; Donoho and Grimes, 2003; Weinberger and Saul, 2006 ).
- Modern NLDR is sometimes referred to as manifold learning; it is also tightly connected with graph embedding ( Di Battista et al., 1999 ) and spectral clustering ( Bengio et al., 2003; Saerens et al., 2004; Nadler et al., 2006; Brand and Huang, 2003 ).
- The reconstruction error is a universal quality criterion, but it requires the availability of M andM/C0 1 in closed form, whereas most NLDR methods are nonparametric (they merely provide values ofM for the known vectors ni).
- Keep preprocessing, initialization policy, and seed protocol fixed when comparing methods or parameter settings.

# Claim Atoms (For Conflict Resolution)
- CLAIM-PENDING-EXTRA-C1 | stance: support | summary: Introduction The interpretation of high-dimensional data remains a difﬁcult task, mainly because human vision is not used to deal with spaces whose dimensionality is higher than three. | evidence_ids: PENDING-EXTRA-E1, PENDING-EXTRA-E2
- CLAIM-PENDING-EXTRA-C2 | stance: support | summary: As most NLDR methods optimize a given objective function, a simplistic way to assess quality is to look at the value of the objective function after convergence. | evidence_ids: PENDING-EXTRA-E3, PENDING-EXTRA-E4
- CLAIM-PENDING-EXTRA-C3 | stance: support | summary: Obviously, this allows us to compare several runs with e.g. different parameter values, but makes the comparison of different methods unfair. | evidence_ids: PENDING-EXTRA-E5, PENDING-EXTRA-E6

# Workflow Relevance Map
- step: 1 | relevance: medium | note: supports explicit task clarification before DR recommendation
- step: 2 | relevance: medium | note: adds preprocessing/data-condition constraints for reliable comparison
- step: 3 | relevance: high | note: directly informs task-aligned method and metric selection
- step: 4 | relevance: high | note: contains initialization sensitivity or control-policy evidence

# Evidence
- PENDING-EXTRA-E1 | page: 1, section: extracted, quote: "Introduction The interpretation of high-dimensional data remains a difﬁcult task, mainly because human vision is not used to deal with spaces whose dimensionality is higher than three."
- PENDING-EXTRA-E2 | page: 1, section: extracted, quote: "If not all distances are speciﬁed, then the problem can elegantly be modeled using a graph, in which edges are present for known entries of the pairwise distance matrix."
- PENDING-EXTRA-E3 | page: 1, section: extracted, quote: "If visualization is difﬁcult in high-dimensional space, perhaps an (almost) equivalent representation in a lowerdimensional space could improve the readability of data."
- PENDING-EXTRA-E4 | page: 1, section: extracted, quote: "abstract Dimensionality reduction aims at representing high-dimensional data in low-dimensional spaces, in order to facilitate their visual interpretation."
- PENDING-EXTRA-E5 | page: 2, section: extracted, quote: "As most NLDR methods optimize a given objective function, a simplistic way to assess quality is to look at the value of the objective function after convergence."
- PENDING-EXTRA-E6 | page: 1, section: extracted, quote: "For the past 25 years, research around NLDR has deeply evolved and after some interest in neural approaches ( Kohonen, 1982; Kramer, 1991; Oja, 1991; Demartines and Hérault, 1993; Mao et al., 1995 ), the community has recently focused on spectral techniques ( Schölkopf et al., 1998; Tenenbaum et al., 2000; Roweis and Saul, 2000; Belkin and Niyogi, 2003; Donoho and Grimes, 2003; Weinberger and Saul, 2006 )."
- PENDING-EXTRA-E7 | page: 1, section: extracted, quote: "Modern NLDR is sometimes referred to as manifold learning; it is also tightly connected with graph embedding ( Di Battista et al., 1999 ) and spectral clustering ( Bengio et al., 2003; Saerens et al., 2004; Nadler et al., 2006; Brand and Huang, 2003 )."
- PENDING-EXTRA-E8 | page: 2, section: extracted, quote: "The reconstruction error is a universal quality criterion, but it requires the availability of M andM/C0 1 in closed form, whereas most NLDR methods are nonparametric (they merely provide values ofM for the known vectors ni)."
