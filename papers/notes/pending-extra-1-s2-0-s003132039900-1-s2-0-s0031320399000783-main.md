---
id: paper-2000-pending-extra-1-s2-0-s003132039900
title: "Data visualization by multidimensional scaling: a deterministic annealing approach"
authors: "Hansjoerg Klock, Joachim M. Buhmann"
venue: "Pattern Recognition"
year: 2000
tags: [dr, reliability, visual_analytics, reference, pending_references]
source_pdf: papers/raw/pending-references/1-s2.0-S0031320399000783-main.pdf
seed_note_id: "paper-2025-jeon-reliable-va-survey"
evidence_level: medium
updated_at: 2026-02-08
---

# Problem
- Stochastic optimization and deterministic annealing Since the introduction ofsimulated annealing in a seminal paper of Kirkpatrick et al. [22], stochastic maximum entropy approaches to optimization problems have found widespread acceptance in the pattern recognition and computer vision community as alternative to gradient descent or other deterministic techniques.
- Buhmann* Rheinische Friedrich-Wilhelms-Universita( t, Institut fu( r Informatik III, Ro( merstra}e 164, D-53117 Bonn, Germany Received 15 March 1999 Abstract Multidimensional scaling addresses the problem how proximity data can be faithfully visualized as points in a low-dimensional Euclidean space.
- Using Eq. (11) we determine the Kullback}Leibler divergence between P0 and the Gibbs density PG D(P0DDPG)" N+ i/1 Slogqi(xi D#i)T#1 „ [SHMDST!F]. (13) The correct free energyF of the system does not depend on the mean"eld parameters and can be neglected in the minimization problem.
- Under this condition it makes sense to formulate MDS as an optimization problem with the cost function H(MxiN)" N+ i/1 N+ k/1 wik(dik!Dik)2. (1) The factors wik are introduced to weight the disparities individually.

# Method Summary
- MDS methods which minimize an objective function of this types are commonly referred to as least squares scaling (LSS) and belong to the class of metric multidimensional scaling algorithms.
- We present a novel deterministic annealing algorithm for the frequently used objective SSTRESS and for Sammon mapping, derived in the framework of maximum entropy estimation.
- To determine the optimal parameters #i, we have to minimize the Kullback }Leibler (KL) divergence between the factorial densityP0(X) and the Gibbs density PG(X), D(P0(XD#)DDPG(X)) , P) P0(XD#)logA P0(XD#) PG(X) BdX. (10) EM-Algorithm: The introduction of the mean "eld parameters #i suggests an alternating algorithm to estimate the expectation values of the embedding coordinates.
- Minimization of the partial costs The minimization of the potentials (25) and (34) de"ning the partial cost of embeddingxi with a local density model qi(xiD#i) plays an important role for the convergence of the algorithm, since a straightforward minimization by Newton}Raphson or conjugate gradient is likely to "nd a local minimum.
- As the algorithms minimize an upper bound on the free energy de"ned by the respective cost function, convergence is guaranteed independently of any critical parameters such as the step-size in the gradient descent approach.
- In simulated annealing, the interesting expectation values of the system parameters, e.g., the expected embedding coordinates in MDS, are estimated by sampling the Gibbs distribution PG using a Monte Carlo method.
- Conclusion A novel algorithm for least}squares multidimensional scaling based on the widely used SSTRESS objective has been developed in the framework of maximum entropy inference [5].

# When To Use / Not Use
- Use when:
  - Depending on the data analysis task at hand, it might be appropriate to use a local, a global or an intermediate normalization w(-)ik" 1 N(N!1)D2ik , w(')ik " 1 +Nl,m/1D2-. , w(.)ik " 1 Dik+Nl,m/1Dlm . (2) The di !erent choices correspond to a minimization of relative, absolute or intermediate error [6].
  - Exact model : The ansatz #i"'i for the factorizing density: q0i (xiD#i)" 1 Z0i expA!1 „ fi(xi)B, (18) Z0i "P = ~= dxiexpA!1 „ fi(xi)B, (19) fi(xi)"a0i DDxiDD4#DDxiDD2xTi hK i#Tr[xixTi H]#xTi hi (20) can be used in principle, since the factorial density is directly parameterized by the statistics'i.
- Avoid when:
  - If y"f(x;W) is the output vector, mapped by a function f which depends on a set of weights W, the stress becomes H(MxiN)" N+ i/1 N+ k/1 wik(DDf(xi; W)!f(xk;W)DD!Dik)2. (6) Di!erentiating with respect toW yields a set of equations that can be used to adjust the weightsW.
  - Deterministic annealing helps to avoid such situations, since the re#ectional symmetry of the clusters is broken in the beginning of the experiment at high temperatures, when the points inside each cluster are still at an entropy-dominated position.
- Failure modes:
  - A comparison of our (zero temperature) results with the solutions produced by the program sammon are summarized as follows: The zero temperature version of the algorithm avoided very poor local minima but produced a broad distribution.

# Metrics Mentioned
- `continuity`: referenced as part of projection-quality or reliability assessment.
- `stress`: referenced as part of projection-quality or reliability assessment.
- `procrustes-based quality`: referenced as part of projection-quality or reliability assessment.
- `kl divergence`: referenced as part of projection-quality or reliability assessment.
- `neighborhood-preservation criteria`: referenced as part of projection-quality or reliability assessment.

# Implementation Notes
- For, e.g. the Iris data set and depending on the convergence parameter e as well as the annealing schedule ( g"0.8 within an exponential schedule), the total execution time of 203 seconds CPU time on a 300 MHz LinuxPentium-II is indeed comparable to the CPU time of sammon (149 s/5000 iterations).
- As for HMDS, the algorithm decreases the computational temperature „ exponentially while iterating in an asynchronous update scheme the estimation of the statistics ' (E-like step) and the optimization of the mean"eld parameters # (M-like step).
- Approximations for Sammon mapping In contrast to the SSTRESS cost function (3), where an optimization algorithm based on local "x-point iterations exists, the usual way to minimize the costs of Sammon mapping is by gradient descent.
- Consequently, the set of stationary equations can be solved to yield a currently optimalxi keeping the other points and the auxiliary parameters "xed, and an iterative optimization scheme analogous to the case of HMDS can be de"ned.
- The algorithm decreases the temperature parameter exponentially while an estimate of the statistics' (E-like step) is alternated with an optimization of the parameters # (M-like step).
- Keep preprocessing, initialization policy, and seed protocol fixed when comparing methods or parameter settings.

# Claim Atoms (For Conflict Resolution)
- CLAIM-PENDING-EXTRA-C1 | stance: support | summary: Stochastic optimization and deterministic annealing Since the introduction ofsimulated annealing in a seminal paper of Kirkpatrick et al. [22], stochastic maximum entropy approaches to optimization problems have found widespread acceptance in the pattern recognition and computer vision community as alternative to gradient descent or other deterministic techniques. | evidence_ids: PENDING-EXTRA-E1, PENDING-EXTRA-E2
- CLAIM-PENDING-EXTRA-C2 | stance: support | summary: MDS methods which minimize an objective function of this types are commonly referred to as least squares scaling (LSS) and belong to the class of metric multidimensional scaling algorithms. | evidence_ids: PENDING-EXTRA-E3, PENDING-EXTRA-E4
- CLAIM-PENDING-EXTRA-C3 | stance: support | summary: For, e.g. the Iris data set and depending on the convergence parameter e as well as the annealing schedule ( g"0.8 within an exponential schedule), the total execution time of 203 seconds CPU time on a 300 MHz LinuxPentium-II is indeed comparable to the CPU time of sammon (149 s/5000 iterations). | evidence_ids: PENDING-EXTRA-E5, PENDING-EXTRA-E6

# Workflow Relevance Map
- step: 1 | relevance: medium | note: supports explicit task clarification before DR recommendation
- step: 2 | relevance: medium | note: adds preprocessing/data-condition constraints for reliable comparison
- step: 3 | relevance: high | note: directly informs task-aligned method and metric selection
- step: 5 | relevance: high | note: provides hyperparameter sensitivity/optimization guidance

# Evidence
- PENDING-EXTRA-E1 | page: 4, section: extracted, quote: "Stochastic optimization and deterministic annealing Since the introduction ofsimulated annealing in a seminal paper of Kirkpatrick et al. [22], stochastic maximum entropy approaches to optimization problems have found widespread acceptance in the pattern recognition and computer vision community as alternative to gradient descent or other deterministic techniques."
- PENDING-EXTRA-E2 | page: 1, section: extracted, quote: "Buhmann* Rheinische Friedrich-Wilhelms-Universita( t, Institut fu( r Informatik III, Ro( merstra}e 164, D-53117 Bonn, Germany Received 15 March 1999 Abstract Multidimensional scaling addresses the problem how proximity data can be faithfully visualized as points in a low-dimensional Euclidean space."
- PENDING-EXTRA-E3 | page: 5, section: extracted, quote: "Using Eq. (11) we determine the Kullback}Leibler divergence between P0 and the Gibbs density PG D(P0DDPG)' N+ i/1 Slogqi(xi D#i)T#1 „ [SHMDST!F]. (13) The correct free energyF of the system does not depend on the mean'eld parameters and can be neglected in the minimization problem."
- PENDING-EXTRA-E4 | page: 2, section: extracted, quote: "Under this condition it makes sense to formulate MDS as an optimization problem with the cost function H(MxiN)' N+ i/1 N+ k/1 wik(dik!Dik)2. (1) The factors wik are introduced to weight the disparities individually."
- PENDING-EXTRA-E5 | page: 2, section: extracted, quote: "MDS methods which minimize an objective function of this types are commonly referred to as least squares scaling (LSS) and belong to the class of metric multidimensional scaling algorithms."
- PENDING-EXTRA-E6 | page: 1, section: extracted, quote: "We present a novel deterministic annealing algorithm for the frequently used objective SSTRESS and for Sammon mapping, derived in the framework of maximum entropy estimation."
- PENDING-EXTRA-E7 | page: 5, section: extracted, quote: "To determine the optimal parameters #i, we have to minimize the Kullback }Leibler (KL) divergence between the factorial densityP0(X) and the Gibbs density PG(X), D(P0(XD#)DDPG(X)) , P) P0(XD#)logA P0(XD#) PG(X) BdX. (10) EM-Algorithm: The introduction of the mean 'eld parameters #i suggests an alternating algorithm to estimate the expectation values of the embedding coordinates."
- PENDING-EXTRA-E8 | page: 17, section: extracted, quote: "Minimization of the partial costs The minimization of the potentials (25) and (34) de'ning the partial cost of embeddingxi with a local density model qi(xiD#i) plays an important role for the convergence of the algorithm, since a straightforward minimization by Newton}Raphson or conjugate gradient is likely to 'nd a local minimum."
