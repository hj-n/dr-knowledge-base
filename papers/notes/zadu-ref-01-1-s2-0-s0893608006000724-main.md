---
id: paper-2006-zadu-ref-01
title: "Local multidimensional scaling"
authors: "Jarkko Venna, Samuel Kaski"
venue: "Neural Networks"
year: 2006
tags: [dr, zadu-table1-reference, tnc]
source_pdf: papers/raw/zadu-table1-references/1-s2.0-S0893608006000724-main.pdf
evidence_level: medium
updated_at: 2026-02-08
---
# Problem
- If preservation of original neighborhoods is required the linear method PCA is a good ﬁrst choice followed by SNE which can produce better results but is computationally heavy, and prone to problems caused by local minima.
- The conﬁguration of points in the low-dimensional space can be found by solving the generalized eigenvalue problem Ly = λDy, (6) where D is the diagonal matrix with elements Dii = ∑ j Wi j, and L = D − W.
- Comparison of the methods We start by comparing the nonlinear projection methods on toy data sets to illustrate their properties in a visualization task, and on real world high-dimensional data sets.
- The latter means has ﬁxed the problem for isomaps, where we had already used a very large number ( k = 67 in comparison to k = 7 (CDA) and k = 4 (SNEG)) of neighbors to make the graph connected.
- The problem can be solved by ﬁnding the p + 1 smallest eigenvalues of the matrix ( I − W) T ( I − W) (details in Roweis and Saul (2000)), where p is the dimensionality of the output.

# Method Summary
- So far the methods, isomap ( Tenenbaum, de Silva, & Langford, 2000), locally linear embedding (LLE: Roweis and Saul (2000)), Laplacian eigenmap ( Belkin & Niyogi, 2002a) and stochastic neighbor embedding (SNE: Hinton and Roweis (2002)), have not been compared in the task of visualization where the dimensionality of the representation is not selected based on the manifold but constrained by the display instead.
- Stochastic neighbor embedding (SNE) The SNE algorithm developed by Hinton and Roweis (2002) does not try to preserve pairwise distances as such, but instead the probabilities of points being neighbors.
- Comparison of the methods We start by comparing the nonlinear projection methods on toy data sets to illustrate their properties in a visualization task, and on real world high-dimensional data sets.
- We introduced an extension of CCA called local MDS, that according to the experimental results is capable of controlling the tradeoff between trustworthiness and continuity of the projection.
- Locally linear embedding (LLE) The LLE algorithm developed by Roweis and Saul (2000) is based on the assumption that we can make a locally linear approximation of the data manifold.
- Box 5400, FI-02015 TKK, Finland Abstract In a visualization task, every nonlinear projection method needs to make a compromise between trustworthiness and continuity.
- Abstract In a visualization task, every nonlinear projection method needs to make a compromise between trustworthiness and continuity.

# When To Use / Not Use
- Use when:
  - Results with local MDS The number of neighbors used in the graph approximation of the geodesic distance was selected to be the same that produced the best result on CDA.
  - When the best matching model vector has been found, the model vectors are updated with m j ( t + 1) = m j ( t) + hc( t), j ( t) [ x( t) − m j ( t) ] .
  - Thus, based on empirical ﬁndings, we recommend that λ should be kept within the range [ 0, 0.5] when Euclidean distances are used.
- Avoid when:
  - 4 on the mouse gene expression data set both versions of local MDS increase in trustworthiness when λ is changed from 0 to 0.1.
  - First, for data point x( t) chosen randomly at iteration step t = 0, 1, 2, . . . , the best matching model vector mc( t) is sought using the equation c( t) = argmin j {d( x( t), m j ) }, (12) where d( x( t), m j ) is the distance between the data point x( t) and the model vector m j .
- Failure modes:
  - Gower (1966) has shown that when the dimensionality of the solutions is the same, the projection of the original data to the PCA subspace equals the conﬁguration of points found by linear MDS that is calculated from the Euclidean distance matrix of the data.
  - Typically a Euclidean distance matrix has been used but there has recently been a trend, started by isomap, of creating new variants of older methods by replacing the Euclidean distance matrix with approximation of the geodesic distance matrix.

# Metrics Mentioned
- `tnc`: Trustworthiness and Continuity behavior.
- `nh`: label-based neighborhood hit.
- `nd`: neighbor-shape dissimilarity or neighbor distortion.
- `kl_div`: Kullback-Leibler divergence behavior.
- `pr`: pairwise-distance correlation behavior.
- `proc`: Procrustes local shape agreement.

# Implementation Notes
- It would be even better to let the user decide about the compromise between trustworthiness and continuity, and in this work we will extend CCA to make a parameterized compromise between trustworthiness and continuity.
- We then extend it to focus on local proximities both in the input and output space, and to explicitly make a user-tunable parameterized compromise between trustworthiness and continuity.
- On the visualization produced by Laplacian eigenmaps the differences in the scales of distances are so large that it is not possible to discern any structure within the clusters.
- The starting point is a random initialization of points in the reduced-dimensional output space, and a pairwise distance matrix between the original data points.
- On this data set, when the parameter λ is zero, trustworthiness is at ﬁrst high at very small neighborhoods but then drops relatively fast.
- Methods that do not have a global optimum were run ten times on each data set, starting from a different random initialization each time.
- Keep preprocessing, initialization policy, and random-seed protocol fixed when comparing methods.

# Claim Atoms (For Conflict Resolution)
- CLAIM-PAPER2006ZADUREF01-C1 | stance: support | summary: If preservation of original neighborhoods is required the linear method PCA is a good ﬁrst choice followed by SNE which can produce better results but is computationally heavy, and prone to problems caused by local minima. | evidence_ids: PAPER2006ZADUREF01-E1, PAPER2006ZADUREF01-E2
- CLAIM-PAPER2006ZADUREF01-C2 | stance: support | summary: So far the methods, isomap ( Tenenbaum, de Silva, & Langford, 2000), locally linear embedding (LLE: Roweis and Saul (2000)), Laplacian eigenmap ( Belkin & Niyogi, 2002a) and stochastic neighbor embedding (SNE: Hinton and Roweis (2002)), have not been compared in the task of visualization where the dimensionality of the representation is not selected based on the manifold but constrained by the display instead. | evidence_ids: PAPER2006ZADUREF01-E3, PAPER2006ZADUREF01-E4
- CLAIM-PAPER2006ZADUREF01-C3 | stance: support | summary: It would be even better to let the user decide about the compromise between trustworthiness and continuity, and in this work we will extend CCA to make a parameterized compromise between trustworthiness and continuity. | evidence_ids: PAPER2006ZADUREF01-E5, PAPER2006ZADUREF01-E6
- CLAIM-METRIC-TNC-SOURCE-01 | stance: support | summary: This source includes evidence relevant to `tnc` in dimensionality-reduction reliability evaluation. | evidence_ids: PAPER2006ZADUREF01-E1, PAPER2006ZADUREF01-E2

# Workflow Relevance Map
- step: 1 | relevance: medium | note: supports task clarification before model selection
- step: 2 | relevance: medium | note: adds constraints for preprocessing and data-audit checks
- step: 3 | relevance: high | note: directly informs task-aligned technique/metric selection
- step: 5 | relevance: medium | note: adds initialization sensitivity guidance
- step: 6 | relevance: high | note: adds hyperparameter sensitivity or optimization guidance

# Evidence
- PAPER2006ZADUREF01-E1 | page: 10, section: extracted, quote: "If preservation of original neighborhoods is required the linear method PCA is a good ﬁrst choice followed by SNE which can produce better results but is computationally heavy, and prone to problems caused by local minima."
- PAPER2006ZADUREF01-E2 | page: 3, section: extracted, quote: "The conﬁguration of points in the low-dimensional space can be found by solving the generalized eigenvalue problem Ly = λDy, (6) where D is the diagonal matrix with elements Dii = ∑ j Wi j, and L = D − W."
- PAPER2006ZADUREF01-E3 | page: 4, section: extracted, quote: "Comparison of the methods We start by comparing the nonlinear projection methods on toy data sets to illustrate their properties in a visualization task, and on real world high-dimensional data sets."
- PAPER2006ZADUREF01-E4 | page: 7, section: extracted, quote: "The latter means has ﬁxed the problem for isomaps, where we had already used a very large number ( k = 67 in comparison to k = 7 (CDA) and k = 4 (SNEG)) of neighbors to make the graph connected."
- PAPER2006ZADUREF01-E5 | page: 3, section: extracted, quote: "The problem can be solved by ﬁnding the p + 1 smallest eigenvalues of the matrix ( I − W) T ( I − W) (details in Roweis and Saul (2000)), where p is the dimensionality of the output."
- PAPER2006ZADUREF01-E6 | page: 2, section: extracted, quote: "The projection directions can be found by solving the eigenvalue problem Cx a = λa, (2) where Cx is the covariance matrix of the data x."
- PAPER2006ZADUREF01-E7 | page: 1, section: extracted, quote: "So far the methods, isomap ( Tenenbaum, de Silva, & Langford, 2000), locally linear embedding (LLE: Roweis and Saul (2000)), Laplacian eigenmap ( Belkin & Niyogi, 2002a) and stochastic neighbor embedding (SNE: Hinton and Roweis (2002)), have not been compared in the task of visualization where the dimensionality of the representation is not selected based on the manifold but constrained by the display instead."
- PAPER2006ZADUREF01-E8 | page: 3, section: extracted, quote: "Stochastic neighbor embedding (SNE) The SNE algorithm developed by Hinton and Roweis (2002) does not try to preserve pairwise distances as such, but instead the probabilities of points being neighbors."
- PAPER2006ZADUREF01-E9 | page: 10, section: extracted, quote: "We introduced an extension of CCA called local MDS, that according to the experimental results is capable of controlling the tradeoff between trustworthiness and continuity of the projection."
- PAPER2006ZADUREF01-E10 | page: 2, section: extracted, quote: "Locally linear embedding (LLE) The LLE algorithm developed by Roweis and Saul (2000) is based on the assumption that we can make a locally linear approximation of the data manifold."
- PAPER2006ZADUREF01-E11 | page: 1, section: extracted, quote: "Box 5400, FI-02015 TKK, Finland Abstract In a visualization task, every nonlinear projection method needs to make a compromise between trustworthiness and continuity."
- PAPER2006ZADUREF01-E12 | page: 1, section: extracted, quote: "Abstract In a visualization task, every nonlinear projection method needs to make a compromise between trustworthiness and continuity."
- PAPER2006ZADUREF01-E13 | page: 2, section: extracted, quote: "Typically a Euclidean distance matrix has been used but there has recently been a trend, started by isomap, of creating new variants of older methods by replacing the Euclidean distance matrix with approximation of the geodesic distance matrix."
- PAPER2006ZADUREF01-E14 | page: 3, section: extracted, quote: "The LLE implementation at http://www.cs.toronto.edu/ ∼ roweis/lle/ was used in the experiments."
